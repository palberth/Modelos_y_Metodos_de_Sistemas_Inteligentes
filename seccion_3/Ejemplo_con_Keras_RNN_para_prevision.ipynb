{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0f6c40",
   "metadata": {},
   "source": [
    "# **Previsión de Series Temporales con RNN (LSTM y GRU) en Keras**\n",
    "## **Dataset: Pasajeros de Aerolínea (Air Passengers)**\n",
    "\n",
    "**Objetivo del Notebook:**\n",
    "Este notebook presenta un flujo de trabajo para construir, entrenar, evaluar y utilizar modelos de Redes Neuronales Recurrentes (RNNs), específicamente LSTM y GRU, para la previsión de series temporales utilizando **Keras (con TensorFlow backend)**.\n",
    "\n",
    "**Estructura del Notebook:**\n",
    "1.  Carga de las librerías necesarias.\n",
    "2.  Carga del dataset de series temporales (Air Passengers desde CSV).\n",
    "3.  Visualización del dataset y Análisis Exploratorio de Datos (EDA).\n",
    "4.  Preprocesamiento del dataset (escalado, creación de secuencias).\n",
    "5.  Definición y configuración de dos modelos RNN (LSTM y GRU) con Keras.\n",
    "6.  Entrenamiento y evaluación de los modelos.\n",
    "7.  Inferencia (previsión) con el mejor modelo.\n",
    "8.  Guardado del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011bbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carga de las Librerías\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# TensorFlow y Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Pandas para la carga y manipulación de datos CSV\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy para operaciones numéricas\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn para preprocesamiento (escalado) y métricas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Matplotlib para visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates # Para formatear fechas en los gráficos\n",
    "\n",
    "# Otras utilidades\n",
    "import os\n",
    "import datetime\n",
    "import math # Para sqrt en RMSE\n",
    "\n",
    "# Comprobación de versiones\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "\n",
    "# Configuraciones opcionales para Matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6] # Ajustar tamaño para series temporales\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Determinar el dispositivo (GPU si está disponible, sino CPU)\n",
    "# Keras/TensorFlow manejan esto más automáticamente.\n",
    "# Podemos verificar la disponibilidad de GPU si es necesario.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nUsando dispositivo: GPU ({len(gpus)} GPUs disponibles)\")\n",
    "    # tf.config.experimental.set_memory_growth(gpus[0], True) # Opcional: para evitar que TF reserve toda la memoria GPU\n",
    "else:\n",
    "    print(\"\\nUsando dispositivo: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9835b19",
   "metadata": {},
   "source": [
    "## **2. Carga del Dataset de Series Temporales**\n",
    "\n",
    "En esta sección, cargaremos el dataset \"Air Passengers\".\n",
    "Asumiremos que está en un archivo CSV llamado `AirPassengers.csv` con columnas \"Month\" y \"#Passengers\".\n",
    "Realizaremos una inspección inicial de los datos.\n",
    "\n",
    "**Nota:** Si no tienes el archivo `AirPassengers.csv`, puedes buscarlo online (es un dataset muy común)\n",
    "o usar otros datasets de series temporales disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e57245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Carga del Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# --- Constantes Globales Clave ---\n",
    "# Esta constante BATCH_SIZE es fundamental y se utiliza en celdas posteriores\n",
    "# (en model.fit())\n",
    "BATCH_SIZE = 32\n",
    "# --- Fin de Constantes Globales Clave ---\n",
    "\n",
    "# Ruta al archivo CSV. Ajusta esta ruta si es necesario.\n",
    "# Comúnmente, este archivo tiene columnas 'Month' y '#Passengers' o similar.\n",
    "csv_path = 'AirPassengers.csv' # ASEGÚRATE DE QUE ESTE ARCHIVO EXISTA EN LA RUTA ESPECIFICADA\n",
    "\n",
    "try:\n",
    "    # Cargar el dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset '{csv_path}' cargado exitosamente.\")\n",
    "\n",
    "    # Inspección inicial\n",
    "    print(\"\\n--- Primeras filas del dataset: ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- Información del DataFrame: ---\")\n",
    "    df.info()\n",
    "\n",
    "    # Asumir que la columna de pasajeros es la segunda (índice 1) si no se llama '#Passengers'\n",
    "    # y la columna de fecha es la primera (índice 0)\n",
    "    if '#Passengers' in df.columns:\n",
    "        passengers_col_name = '#Passengers'\n",
    "    elif 'Passengers' in df.columns:\n",
    "        passengers_col_name = 'Passengers'\n",
    "    else:\n",
    "        print(f\"Advertencia: No se encontró la columna '#Passengers' o 'Passengers'. Usando la segunda columna: {df.columns[1]}\")\n",
    "        passengers_col_name = df.columns[1]\n",
    "\n",
    "    if 'Month' in df.columns:\n",
    "        date_col_name = 'Month'\n",
    "    else:\n",
    "        print(f\"Advertencia: No se encontró la columna 'Month'. Usando la primera columna: {df.columns[0]}\")\n",
    "        date_col_name = df.columns[0]\n",
    "\n",
    "    # Convertir la columna de fecha a datetime y establecerla como índice\n",
    "    df[date_col_name] = pd.to_datetime(df[date_col_name])\n",
    "    df.set_index(date_col_name, inplace=True)\n",
    "\n",
    "    # Seleccionar solo la serie de pasajeros\n",
    "    time_series_data = df[passengers_col_name]\n",
    "\n",
    "    print(f\"\\n--- Serie temporal de '{passengers_col_name}' (primeras filas): ---\")\n",
    "    print(time_series_data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{csv_path}' no fue encontrado.\")\n",
    "    print(\"Por favor, descarga el dataset 'AirPassengers.csv' y colócalo en la ruta correcta o actualiza 'csv_path'.\")\n",
    "    # Crear un DataFrame de ejemplo si el archivo no se encuentra, para que el resto del notebook pueda ejecutarse parcialmente.\n",
    "    print(\"Creando un dataset de ejemplo para demostración...\")\n",
    "    dates_example = pd.date_range(start='1949-01-01', periods=144, freq='MS') # 'MS' para inicio de mes\n",
    "    values_example = np.linspace(100, 600, 144) + np.random.randn(144)*20 + np.sin(np.arange(144)/6)*50\n",
    "    values_example = np.maximum(50, values_example).astype(int) # asegurar positivos\n",
    "    df = pd.DataFrame({'Month': dates_example, '#Passengers': values_example})\n",
    "    df['Month'] = pd.to_datetime(df['Month'])\n",
    "    df.set_index('Month', inplace=True)\n",
    "    passengers_col_name = '#Passengers'\n",
    "    time_series_data = df[passengers_col_name]\n",
    "    print(\"Dataset de ejemplo creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b47dcc",
   "metadata": {},
   "source": [
    "## **3. Visualización del Dataset y Análisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "Visualizaremos la serie temporal para identificar patrones como tendencia, estacionalidad y posibles anomalías.\n",
    "También verificaremos si hay valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualización del Dataset y EDA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if 'time_series_data' in locals():\n",
    "    # 3.1 Visualización de la Serie Temporal Completa\n",
    "    print(\"\\n--- 3.1 Visualización de la Serie Temporal Completa ---\")\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(time_series_data.index, time_series_data.values, label=f'Número de {passengers_col_name}')\n",
    "    plt.title(f'Serie Temporal: {passengers_col_name} (1949-1960)', fontsize=16)\n",
    "    plt.xlabel('Fecha', fontsize=12)\n",
    "    plt.ylabel(f'Número de {passengers_col_name}', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    # Formatear el eje x para mostrar años\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator(2)) # Marcas cada 2 años\n",
    "    plt.show()\n",
    "\n",
    "    # 3.2 Verificación de Valores Faltantes\n",
    "    print(\"\\n--- 3.2 Verificación de Valores Faltantes ---\")\n",
    "    missing_values = time_series_data.isnull().sum()\n",
    "    print(f\"Número de valores faltantes en la serie: {missing_values}\")\n",
    "    if missing_values > 0:\n",
    "        print(\"Advertencia: Existen valores faltantes. Se deberían tratar (ej. interpolación, eliminación).\")\n",
    "        # Para este ejemplo, no implementaremos el tratamiento de faltantes.\n",
    "else:\n",
    "    print(\"No se pudo cargar `time_series_data`. Saltando EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de04913",
   "metadata": {},
   "source": [
    "## **4. Preprocesamiento del Dataset**\n",
    "\n",
    "El preprocesamiento para modelos RNN de series temporales generalmente incluye:\n",
    "1.  **Escalado de Datos:** Los modelos de redes neuronales suelen funcionar mejor con datos escalados (ej. entre 0 y 1 o -1 y 1). Usaremos `MinMaxScaler`.\n",
    "2.  **División en Entrenamiento y Prueba:** Es crucial que la división sea cronológica para series temporales. No se deben mezclar datos del futuro en el conjunto de entrenamiento.\n",
    "3.  **Creación de Secuencias:** Las RNNs (LSTM/GRU) procesan secuencias de datos. Necesitamos transformar nuestra serie temporal en pares de (secuencia_de_entrada, valor_objetivo).\n",
    "    * `sequence_length` (o lookback): Número de pasos temporales pasados que se usarán como entrada.\n",
    "    * `target_steps` (o horizon): Número de pasos futuros a predecir (aquí, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1e824",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 4. Preprocesamiento del Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if 'time_series_data' in locals():\n",
    "    # 4.1 Escalado de Datos\n",
    "    print(\"\\n--- 4.1 Escalado de Datos ---\")\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) # Escalar a [-1, 1] es común para LSTMs\n",
    "    # scaler = MinMaxScaler(feature_range=(0, 1)) # Escalar a [0, 1] también es una opción\n",
    "\n",
    "    # El método fit_transform espera un array 2D, así que convertimos la serie 1D\n",
    "    data_scaled = scaler.fit_transform(time_series_data.values.reshape(-1, 1))\n",
    "    print(f\"Forma de los datos escalados: {data_scaled.shape}\")\n",
    "    print(f\"Primeros 5 valores escalados:\\n{data_scaled[:5]}\")\n",
    "\n",
    "    # 4.2 División en Entrenamiento y Prueba (Cronológica)\n",
    "    print(\"\\n--- 4.2 División en Entrenamiento y Prueba ---\")\n",
    "    # Por ejemplo, usar el 80% para entrenamiento y el 20% para prueba\n",
    "    train_size = int(len(data_scaled) * 0.80)\n",
    "    test_size = len(data_scaled) - train_size\n",
    "\n",
    "    train_data_scaled = data_scaled[0:train_size, :]\n",
    "    test_data_scaled = data_scaled[train_size:len(data_scaled), :]\n",
    "\n",
    "    print(f\"Tamaño del conjunto de entrenamiento escalado: {len(train_data_scaled)}\")\n",
    "    print(f\"Tamaño del conjunto de prueba escalado: {len(test_data_scaled)}\")\n",
    "\n",
    "    # 4.3 Creación de Secuencias\n",
    "    print(\"\\n--- 4.3 Creación de Secuencias ---\")\n",
    "    def create_sequences(input_data, sequence_length, target_steps=1):\n",
    "        \"\"\"\n",
    "        Crea secuencias de entrada y etiquetas objetivo para modelos RNN.\n",
    "        Args:\n",
    "            input_data (np.array): Array de la serie temporal (ya escalada).\n",
    "            sequence_length (int): Número de pasos temporales pasados a usar como entrada.\n",
    "            target_steps (int): Número de pasos futuros a predecir. Default 1.\n",
    "        Returns:\n",
    "            tuple: (np.array de secuencias de entrada, np.array de etiquetas objetivo)\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(input_data) - sequence_length - (target_steps - 1)):\n",
    "            # La secuencia de entrada\n",
    "            seq_in = input_data[i:(i + sequence_length), 0] # Tomar 'sequence_length' valores\n",
    "            # La etiqueta objetivo (el/los siguiente/s valor/es)\n",
    "            seq_out = input_data[(i + sequence_length):(i + sequence_length + target_steps), 0]\n",
    "            X.append(seq_in)\n",
    "            y.append(seq_out)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    SEQUENCE_LENGTH = 12  # Usar los últimos 12 meses para predecir el siguiente\n",
    "    TARGET_STEPS = 1      # Predecir 1 mes adelante\n",
    "    NUM_FEATURES = 1      # Serie temporal univariada\n",
    "\n",
    "    X_train, y_train = create_sequences(train_data_scaled, SEQUENCE_LENGTH, TARGET_STEPS)\n",
    "    X_test, y_test = create_sequences(test_data_scaled, SEQUENCE_LENGTH, TARGET_STEPS)\n",
    "\n",
    "    print(f\"Forma de X_train: {X_train.shape}\") # Debería ser (num_samples, sequence_length)\n",
    "    print(f\"Forma de y_train: {y_train.shape}\")   # Debería ser (num_samples, target_steps)\n",
    "    print(f\"Forma de X_test: {X_test.shape}\")\n",
    "    print(f\"Forma de y_test: {y_test.shape}\")\n",
    "\n",
    "    # Ajustar la forma de X para Keras RNNs: (num_samples, sequence_length, num_features)\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], NUM_FEATURES)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], NUM_FEATURES)\n",
    "    # y_train e y_test ya tienen la forma correcta (num_samples, target_steps) para Dense(target_steps)\n",
    "\n",
    "    print(f\"\\nForma de X_train para Keras: {X_train.shape}\")\n",
    "    print(f\"Forma de y_train para Keras: {y_train.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"No se pudo cargar `time_series_data`. Saltando Preprocesamiento.\")\n",
    "    # Definir arrays vacíos para que el resto del notebook no falle catastróficamente\n",
    "    X_train, y_train, X_test, y_test = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "\n",
    "print(\"\\n--- Fin de Preprocesamiento ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97f2d3",
   "metadata": {},
   "source": [
    "## **5. Definición de Modelos RNN (LSTM y GRU) con Keras**\n",
    "\n",
    "Definiremos dos modelos basados en RNN usando Keras `Sequential` API:\n",
    "1.  **Modelo LSTM:** Utilizará una o más capas LSTM seguidas de una capa Densa para la predicción.\n",
    "2.  **Modelo GRU:** Similar, pero utilizando capas GRU.\n",
    "\n",
    "Ambos modelos tomarán secuencias de entrada y predecirán el valor del siguiente paso temporal.\n",
    "Las capas LSTM y GRU en Keras esperan la entrada como `(batch_size, timesteps, input_features)`.\n",
    "Nuestra `input_features` (número de features por paso temporal) es 1 para esta serie univariada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Definición de Modelos RNN (LSTM y GRU) con Keras\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "INPUT_SIZE_RNN = NUM_FEATURES # Número de features en cada paso temporal (1 para univariada)\n",
    "HIDDEN_SIZE_RNN = 64 # Número de neuronas en la capa oculta de LSTM/GRU\n",
    "# NUM_LAYERS_RNN = 1   # Keras LSTM/GRU lo maneja apilando capas si es necesario\n",
    "OUTPUT_SIZE_RNN = TARGET_STEPS # Predecir `TARGET_STEPS` adelante (aquí 1)\n",
    "# SEQUENCE_LENGTH ya está definido\n",
    "\n",
    "def crear_modelo_lstm_keras(input_shape, hidden_units, output_units, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_units, input_shape=input_shape, dropout=dropout_rate, recurrent_dropout=dropout_rate)) # recurrent_dropout solo si GPU lo soporta eficientemente\n",
    "    # Si NUM_LAYERS_RNN > 1, se añadirían más capas LSTM con return_sequences=True (excepto la última LSTM)\n",
    "    # model.add(LSTM(hidden_units, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "    # model.add(LSTM(hidden_units, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "    model.add(Dense(output_units))\n",
    "    return model\n",
    "\n",
    "def crear_modelo_gru_keras(input_shape, hidden_units, output_units, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(hidden_units, input_shape=input_shape, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "    model.add(Dense(output_units))\n",
    "    return model\n",
    "\n",
    "# Definir input_shape para la primera capa\n",
    "input_shape_rnn = (SEQUENCE_LENGTH, INPUT_SIZE_RNN)\n",
    "\n",
    "# Instanciar los modelos\n",
    "# El dropout en Keras LSTM/GRU se aplica a las conexiones recurrentes si recurrent_dropout > 0\n",
    "# y a las conexiones de entrada si dropout > 0.\n",
    "# Para una sola capa, el dropout en PyTorch nn.LSTM/GRU solo se aplicaba si num_layers > 1\n",
    "# Mantendremos dropout=0 para una sola capa para emular el comportamiento de PyTorch\n",
    "# Si se usaran múltiples capas apiladas (NUM_LAYERS_RNN > 1), se podría añadir dropout.\n",
    "dropout_val = 0.2 if False else 0.0 # Cambiar a True para activar dropout si se apilan capas\n",
    "\n",
    "modelo_lstm_keras = crear_modelo_lstm_keras(input_shape_rnn, HIDDEN_SIZE_RNN, OUTPUT_SIZE_RNN, dropout_rate=dropout_val)\n",
    "modelo_gru_keras = crear_modelo_gru_keras(input_shape_rnn, HIDDEN_SIZE_RNN, OUTPUT_SIZE_RNN, dropout_rate=dropout_val)\n",
    "\n",
    "print(\"--- Modelo LSTM (Keras) ---\")\n",
    "modelo_lstm_keras.summary()\n",
    "\n",
    "print(\"\\n--- Modelo GRU (Keras) ---\")\n",
    "modelo_gru_keras.summary()\n",
    "\n",
    "print(\"\\nModelos RNN (Keras) definidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55a874",
   "metadata": {},
   "source": [
    "## **6. Entrenamiento y Evaluación de los Modelos (Keras)**\n",
    "\n",
    "El proceso con Keras es más directo:\n",
    "1.  Compilar el modelo: Especificar optimizador, función de pérdida y métricas.\n",
    "    * Función de pérdida: `mean_squared_error` (MSE).\n",
    "    * Optimizador: `Adam`.\n",
    "    * Métricas: `mse`.\n",
    "2.  Entrenar el modelo usando `model.fit()`, que devuelve un objeto `history`.\n",
    "3.  Evaluar el modelo usando `model.evaluate()`.\n",
    "4.  Graficar las curvas de pérdida de entrenamiento y validación del objeto `history`.\n",
    "\n",
    "Las métricas principales serán MSE y RMSE (Raíz del Error Cuadrático Medio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb4563",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 6. Entrenamiento y Evaluación de los Modelos RNN (Keras)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "criterion_keras = 'mean_squared_error'\n",
    "\n",
    "def entrenar_y_evaluar_rnn_keras(modelo_rnn, train_X_data, train_y_data, test_X_data, test_y_data,\n",
    "                                criterio_loss_str, optimizador_rnn, num_epochs_val, batch_size_val,\n",
    "                                nombre_modelo_str_val):\n",
    "    print(f\"\\n--- Entrenando y Evaluando: {nombre_modelo_str_val} ---\")\n",
    "\n",
    "    modelo_rnn.compile(optimizer=optimizador_rnn, loss=criterio_loss_str, metrics=['mse'])\n",
    "\n",
    "    # El entrenamiento se realiza directamente con los arrays NumPy\n",
    "    history = modelo_rnn.fit(\n",
    "        train_X_data, train_y_data,\n",
    "        epochs=num_epochs_val,\n",
    "        batch_size=batch_size_val,\n",
    "        validation_data=(test_X_data, test_y_data),\n",
    "        verbose=1, # 0 para silencioso, 1 para barra de progreso, 2 para una línea por época\n",
    "        shuffle=True # shuffle=True es bueno para entrenamiento\n",
    "    )\n",
    "\n",
    "    # Graficar historial de pérdida\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Pérdida Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida Validación')\n",
    "    plt.title(f\"Historial de Pérdida: {nombre_modelo_str_val}\", fontsize=16)\n",
    "    plt.xlabel('Época', fontsize=12); plt.ylabel('Pérdida (MSE)', fontsize=12)\n",
    "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.7); plt.show()\n",
    "\n",
    "    # Encontrar la mejor pérdida de validación y la época\n",
    "    best_val_loss_keras = min(history.history['val_loss'])\n",
    "    best_epoch_keras = history.history['val_loss'].index(best_val_loss_keras) + 1\n",
    "    best_val_rmse_keras = math.sqrt(best_val_loss_keras)\n",
    "\n",
    "    print(f\"\\nMejor resultado para {nombre_modelo_str_val} en época {best_epoch_keras}: \"\n",
    "          f\"Pérdida Val (MSE) = {best_val_loss_keras:.6f}, RMSE Val = {best_val_rmse_keras:.4f}\")\n",
    "\n",
    "    return modelo_rnn, history, best_val_loss_keras, best_val_rmse_keras\n",
    "\n",
    "\n",
    "EPOCHS_RNN = 150 # Aumentar para mejores resultados, ej. 100-200\n",
    "LEARNING_RATE_RNN = 0.001\n",
    "\n",
    "if X_train.size > 0: # Solo si hay datos\n",
    "    # Entrenar Modelo LSTM\n",
    "    optimizer_lstm_keras = Adam(learning_rate=LEARNING_RATE_RNN)\n",
    "    modelo_lstm_keras, hist_lstm_keras, loss_lstm_keras, rmse_lstm_keras = entrenar_y_evaluar_rnn_keras(\n",
    "        modelo_lstm_keras, X_train, y_train, X_test, y_test,\n",
    "        criterion_keras, optimizer_lstm_keras,\n",
    "        EPOCHS_RNN, BATCH_SIZE, \"LSTM (Keras)\"\n",
    "    )\n",
    "\n",
    "    # Entrenar Modelo GRU\n",
    "    optimizer_gru_keras = Adam(learning_rate=LEARNING_RATE_RNN)\n",
    "    modelo_gru_keras, hist_gru_keras, loss_gru_keras, rmse_gru_keras = entrenar_y_evaluar_rnn_keras(\n",
    "        modelo_gru_keras, X_train, y_train, X_test, y_test,\n",
    "        criterion_keras, optimizer_gru_keras,\n",
    "        EPOCHS_RNN, BATCH_SIZE, \"GRU (Keras)\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Resumen de Resultados (Keras - Series Temporales) ---\")\n",
    "    print(f\"LSTM: Mejor Pérdida Val (MSE) = {loss_lstm_keras:.6f}, RMSE Val = {rmse_lstm_keras:.4f}\")\n",
    "    print(f\"GRU:  Mejor Pérdida Val (MSE) = {loss_gru_keras:.6f}, RMSE Val = {rmse_gru_keras:.4f}\")\n",
    "\n",
    "    # Seleccionar el mejor modelo\n",
    "    if loss_lstm_keras < loss_gru_keras: # Menor MSE es mejor\n",
    "        mejor_modelo_rnn_keras = modelo_lstm_keras\n",
    "        nombre_mejor_modelo_rnn_keras = \"LSTM (Keras)\"\n",
    "        rmse_mejor_modelo_rnn_keras = rmse_lstm_keras\n",
    "    else:\n",
    "        mejor_modelo_rnn_keras = modelo_gru_keras\n",
    "        nombre_mejor_modelo_rnn_keras = \"GRU (Keras)\"\n",
    "        rmse_mejor_modelo_rnn_keras = rmse_gru_keras\n",
    "    print(f\"\\nEl mejor modelo RNN (Keras) es: {nombre_mejor_modelo_rnn_keras} con RMSE Val = {rmse_mejor_modelo_rnn_keras:.4f}\")\n",
    "else:\n",
    "    print(\"No se pudieron entrenar los modelos RNN debido a datos de entrenamiento vacíos.\")\n",
    "    mejor_modelo_rnn_keras, nombre_mejor_modelo_rnn_keras = None, \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bed494",
   "metadata": {},
   "source": [
    "## **7. Inferencia (Previsión) con el Mejor Modelo (Keras)**\n",
    "\n",
    "Usaremos el mejor modelo RNN entrenado para hacer previsiones sobre el conjunto de prueba.\n",
    "\n",
    "Los pasos son:\n",
    "1. Usar `model.predict()` sobre los datos de prueba (`X_test`).\n",
    "2. **Importante:** Desescalar las predicciones y los valores verdaderos para interpretarlos en la escala original de pasajeros.\n",
    "3. Graficar las predicciones contra los valores reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Inferencia (Previsión) con el Mejor Modelo (Keras)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if mejor_modelo_rnn_keras is not None and X_test.size > 0 and 'scaler' in locals():\n",
    "    print(f\"Usando: {nombre_mejor_modelo_rnn_keras} para la previsión.\")\n",
    "    \n",
    "    # Obtener todas las predicciones del conjunto de prueba\n",
    "    test_preds_scaled_np = mejor_modelo_rnn_keras.predict(X_test)\n",
    "    # y_test ya son los labels escalados en formato NumPy\n",
    "\n",
    "    # Desescalar\n",
    "    # `scaler` fue ajustado con .values.reshape(-1, 1), así que las predicciones también necesitan esa forma\n",
    "    # y luego se desescalan.\n",
    "    # Si y_test es (num_samples, 1), entonces test_preds_scaled_np y y_test (labels) también lo son.\n",
    "    test_preds_descaled = scaler.inverse_transform(test_preds_scaled_np)\n",
    "    test_labels_descaled = scaler.inverse_transform(y_test) # y_test fue creado de test_data_scaled\n",
    "\n",
    "    # Calcular RMSE en la escala original\n",
    "    rmse_original_scale = math.sqrt(mean_squared_error(test_labels_descaled, test_preds_descaled))\n",
    "    print(f\"RMSE de previsión en escala original: {rmse_original_scale:.2f} pasajeros\")\n",
    "\n",
    "    # Graficar las previsiones vs los valores reales\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(test_labels_descaled, label='Valores Reales (Prueba)', color='blue', marker='.')\n",
    "    plt.plot(test_preds_descaled, label='Predicciones del Modelo (Prueba)', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Previsión de Pasajeros: {nombre_mejor_modelo_rnn_keras}', fontsize=16)\n",
    "    plt.xlabel(f'Índice de Tiempo (en conjunto de prueba, desde {train_size - SEQUENCE_LENGTH + (TARGET_STEPS-1) })', fontsize=12)\n",
    "    plt.ylabel(f'Número de {passengers_col_name}', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # Graficar sobre la serie temporal original (parte de prueba)\n",
    "    # Necesitamos los índices de fecha correctos para la parte de prueba\n",
    "    # El primer valor de y_test corresponde al índice (train_size + SEQUENCE_LENGTH) de la serie original\n",
    "    start_idx_test_original = train_size + SEQUENCE_LENGTH + (TARGET_STEPS -1) # Ajuste para el índice inicial de la etiqueta\n",
    "    \n",
    "    # Asegurarse que la longitud de test_dates coincide con test_labels_descaled\n",
    "    end_idx_test_original = start_idx_test_original + len(test_labels_descaled)\n",
    "    test_dates = time_series_data.index[start_idx_test_original : end_idx_test_original]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(time_series_data.index, time_series_data.values, label=\"Serie Original Completa\", alpha=0.5)\n",
    "    if len(test_dates) == len(test_labels_descaled):\n",
    "        plt.plot(test_dates, test_labels_descaled, label='Valores Reales (Prueba)', color='blue', marker='.')\n",
    "        plt.plot(test_dates, test_preds_descaled, label='Predicciones del Modelo (Prueba)', color='red', linestyle='--')\n",
    "    else:\n",
    "        print(\"Advertencia: Discrepancia de longitud entre test_dates y predicciones/etiquetas desescaladas. No se graficará la superposición detallada.\")\n",
    "        # Graficar solo las predicciones con índices numéricos si las fechas no coinciden\n",
    "        plt.plot(range(len(test_labels_descaled)), test_labels_descaled, label='Valores Reales (Prueba) - Índice numérico', color='blue', marker='.')\n",
    "        plt.plot(range(len(test_preds_descaled)), test_preds_descaled, label='Predicciones (Prueba) - Índice numérico', color='red', linestyle='--')\n",
    "\n",
    "\n",
    "    plt.title(f'Previsión Superpuesta en Serie Original: {nombre_mejor_modelo_rnn_keras}', fontsize=16)\n",
    "    plt.xlabel('Fecha', fontsize=12)\n",
    "    plt.ylabel(f'Número de {passengers_col_name}', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=12)) # Marcas cada 12 meses\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se puede realizar inferencia: modelo no entrenado, datos de prueba no disponibles o scaler no definido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931746ce",
   "metadata": {},
   "source": [
    "## **8. Guardar el Mejor Modelo (Keras)**\n",
    "\n",
    "Guardaremos el modelo completo del mejor modelo RNN entrenado usando el formato nativo de Keras (`.keras`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Guardar el Mejor Modelo (Keras)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if mejor_modelo_rnn_keras is not None:\n",
    "    modelos_rnn_guardados_dir_keras = \"modelos_rnn_keras_guardados\"\n",
    "    if not os.path.exists(modelos_rnn_guardados_dir_keras):\n",
    "        os.makedirs(modelos_rnn_guardados_dir_keras)\n",
    "\n",
    "    timestamp_rnn_keras = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    nombre_archivo_limpio_rnn_keras = nombre_mejor_modelo_rnn_keras.replace(\" (Keras)\", \"\").replace(\" \", \"_\")\n",
    "    nombre_archivo_modelo_rnn_keras = f\"{nombre_archivo_limpio_rnn_keras}_{timestamp_rnn_keras}_ts.keras\" # Usar extensión .keras\n",
    "    ruta_guardado_modelo_rnn_keras = os.path.join(modelos_rnn_guardados_dir_keras, nombre_archivo_modelo_rnn_keras)\n",
    "\n",
    "    print(f\"Guardando el modelo completo ({nombre_mejor_modelo_rnn_keras}) en: {ruta_guardado_modelo_rnn_keras}\")\n",
    "    mejor_modelo_rnn_keras.save(ruta_guardado_modelo_rnn_keras)\n",
    "    print(\"Modelo RNN (Keras) guardado exitosamente.\")\n",
    "\n",
    "    # print(\"\\nEjemplo de cómo cargar el modelo RNN (Keras) (comentado):\")\n",
    "    # from tensorflow.keras.models import load_model\n",
    "    # modelo_cargado_rnn_keras = load_model(ruta_guardado_modelo_rnn_keras)\n",
    "    # print(f\"Modelo RNN '{nombre_mejor_modelo_rnn_keras}' cargado y listo para inferencia.\")\n",
    "    # modelo_cargado_rnn_keras.summary()\n",
    "else:\n",
    "    print(\"No hay un mejor modelo RNN para guardar (posiblemente por fallo en carga de datos o entrenamiento).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42c1d8",
   "metadata": {},
   "source": [
    "## **Conclusiones y Próximos Pasos (RNN para Series Temporales en Keras)**\n",
    "\n",
    "En este notebook, hemos construido un sistema de previsión de series temporales usando LSTMs y GRUs en Keras:\n",
    "1.  Cargamos y exploramos el dataset \"Air Passengers\".\n",
    "2.  Preprocesamos los datos: escalado y creación de secuencias de entrada-salida.\n",
    "3.  Definimos modelos LSTM y GRU usando la API Sequential de Keras.\n",
    "4.  Compilamos y entrenamos los modelos usando `model.fit()`.\n",
    "5.  Realizamos previsiones y las comparamos con los valores reales, desescalando para la interpretación.\n",
    "6.  Guardamos el modelo completo.\n",
    "\n",
    "**Posibles Próximos Pasos:**\n",
    "* **Multivariado:** Adaptar para series temporales multivariadas (múltiples features de entrada). `NUM_FEATURES` cambiaría.\n",
    "* **Múltiples Pasos Futuros:** Modificar los modelos (`OUTPUT_SIZE_RNN`) y la creación de secuencias para predecir más de un paso adelante a la vez.\n",
    "* **Atención:** Incorporar capas de atención (`tf.keras.layers.Attention` o `MultiHeadAttention`).\n",
    "* **Modelos Transformer:** Explorar arquitecturas basadas en Transformers.\n",
    "* **Hiperparámetros:** Ajustar `SEQUENCE_LENGTH`, `HIDDEN_SIZE_RNN`, número de capas, tasa de aprendizaje, optimizador, etc., usando herramientas como KerasTuner.\n",
    "* **Callbacks:** Usar callbacks de Keras como `EarlyStopping` y `ModelCheckpoint` para mejorar el entrenamiento.\n",
    "* **Análisis de Errores:** Investigar más a fondo los errores de predicción (ej. residuos).\n",
    "* **Incorporar Features Exógenas:** Si hay variables adicionales que puedan influir en la serie temporal (ej. festivos, promociones)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
