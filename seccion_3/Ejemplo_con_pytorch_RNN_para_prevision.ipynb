{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmxKkHg0Kway"
   },
   "source": [
    "# **Previsión de Series Temporales con RNN (LSTM y GRU) en PyTorch**\n",
    "## **Dataset: Pasajeros de Aerolínea (Air Passengers)**\n",
    "\n",
    "**Objetivo del Notebook:**\n",
    "Este notebook presenta un flujo de trabajo para construir, entrenar, evaluar y utilizar modelos de Redes Neuronales Recurrentes (RNNs), específicamente LSTM y GRU, para la previsión de series temporales utilizando **PyTorch**.\n",
    "\n",
    "**Estructura del Notebook:**\n",
    "1.  Carga de las librerías necesarias.\n",
    "2.  Carga del dataset de series temporales (Air Passengers desde CSV).\n",
    "3.  Visualización del dataset y Análisis Exploratorio de Datos (EDA).\n",
    "4.  Preprocesamiento del dataset (escalado, creación de secuencias).\n",
    "5.  Creación de los `DataLoader` de PyTorch.\n",
    "6.  Definición y configuración de dos modelos RNN (LSTM y GRU).\n",
    "7.  Entrenamiento y evaluación de los modelos.\n",
    "8.  Inferencia (previsión) con el mejor modelo.\n",
    "9.  Guardado del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6385,
     "status": "ok",
     "timestamp": 1746647639064,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "nTCzlribLDH6",
    "outputId": "45b95e99-34f0-4533-b79f-ac1438dccff6"
   },
   "outputs": [],
   "source": [
    "# 1. Carga de las Librerías\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Pandas para la carga y manipulación de datos CSV\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy para operaciones numéricas\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn para preprocesamiento (escalado) y métricas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Matplotlib para visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates # Para formatear fechas en los gráficos\n",
    "\n",
    "# Otras utilidades\n",
    "import os\n",
    "import datetime\n",
    "import math # Para sqrt en RMSE\n",
    "\n",
    "# Comprobación de versiones\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "# print(f\"Scikit-learn Version: {sklearn.__version__}\") # Si se importa sklearn directamente\n",
    "\n",
    "# Configuraciones opcionales para Matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6] # Ajustar tamaño para series temporales\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Determinar el dispositivo (GPU si está disponible, sino CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh2LM2cwNjGH"
   },
   "source": [
    "## **2. Carga del Dataset de Series Temporales**\n",
    "\n",
    "En esta sección, cargaremos el dataset \"Air Passengers\".\n",
    "Asumiremos que está en un archivo CSV llamado `AirPassengers.csv` con columnas \"Month\" y \"#Passengers\".\n",
    "Realizaremos una inspección inicial de los datos.\n",
    "\n",
    "**Nota:** Si no tienes el archivo `AirPassengers.csv`, puedes buscarlo online (es un dataset muy común)\n",
    "o usar otros datasets de series temporales disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1746647639081,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "x1AtxsJeN3P3",
    "outputId": "1c7b0ccd-0649-4879-9930-3ef13faf7708"
   },
   "outputs": [],
   "source": [
    "# 2. Carga del Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# --- Constantes Globales Clave ---\n",
    "# Esta constante BATCH_SIZE es fundamental y se utiliza en celdas posteriores (como la Celda 10)\n",
    "# para crear los DataLoaders.\n",
    "# ¡ASEGÚRATE DE QUE ESTA CELDA SE EJECUTE ANTES DE LAS CELDAS QUE LA NECESITAN!\n",
    "BATCH_SIZE = 32\n",
    "# --- Fin de Constantes Globales Clave ---\n",
    "\n",
    "# Ruta al archivo CSV. Ajusta esta ruta si es necesario.\n",
    "# Comúnmente, este archivo tiene columnas 'Month' y '#Passengers' o similar.\n",
    "csv_path = 'AirPassengers.csv' # ASEGÚRATE DE QUE ESTE ARCHIVO EXISTA EN LA RUTA ESPECIFICADA\n",
    "\n",
    "try:\n",
    "    # Cargar el dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset '{csv_path}' cargado exitosamente.\")\n",
    "\n",
    "    # Inspección inicial\n",
    "    print(\"\\n--- Primeras filas del dataset: ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- Información del DataFrame: ---\")\n",
    "    df.info()\n",
    "\n",
    "    # Asumir que la columna de pasajeros es la segunda (índice 1) si no se llama '#Passengers'\n",
    "    # y la columna de fecha es la primera (índice 0)\n",
    "    if '#Passengers' in df.columns:\n",
    "        passengers_col_name = '#Passengers'\n",
    "    elif 'Passengers' in df.columns:\n",
    "        passengers_col_name = 'Passengers'\n",
    "    else:\n",
    "        print(f\"Advertencia: No se encontró la columna '#Passengers' o 'Passengers'. Usando la segunda columna: {df.columns[1]}\")\n",
    "        passengers_col_name = df.columns[1]\n",
    "\n",
    "    if 'Month' in df.columns:\n",
    "        date_col_name = 'Month'\n",
    "    else:\n",
    "        print(f\"Advertencia: No se encontró la columna 'Month'. Usando la primera columna: {df.columns[0]}\")\n",
    "        date_col_name = df.columns[0]\n",
    "\n",
    "    # Convertir la columna de fecha a datetime y establecerla como índice\n",
    "    df[date_col_name] = pd.to_datetime(df[date_col_name])\n",
    "    df.set_index(date_col_name, inplace=True)\n",
    "\n",
    "    # Seleccionar solo la serie de pasajeros\n",
    "    time_series_data = df[passengers_col_name]\n",
    "\n",
    "    print(f\"\\n--- Serie temporal de '{passengers_col_name}' (primeras filas): ---\")\n",
    "    print(time_series_data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{csv_path}' no fue encontrado.\")\n",
    "    print(\"Por favor, descarga el dataset 'AirPassengers.csv' y colócalo en la ruta correcta o actualiza 'csv_path'.\")\n",
    "    # Crear un DataFrame de ejemplo si el archivo no se encuentra, para que el resto del notebook pueda ejecutarse parcialmente.\n",
    "    print(\"Creando un dataset de ejemplo para demostración...\")\n",
    "    dates_example = pd.date_range(start='1949-01-01', periods=144, freq='MS')\n",
    "    values_example = np.linspace(100, 600, 144) + np.random.randn(144)*20 + np.sin(np.arange(144)/6)*50\n",
    "    values_example = np.maximum(50, values_example).astype(int) # asegurar positivos\n",
    "    df = pd.DataFrame({'Month': dates_example, '#Passengers': values_example})\n",
    "    df['Month'] = pd.to_datetime(df['Month'])\n",
    "    df.set_index('Month', inplace=True)\n",
    "    passengers_col_name = '#Passengers'\n",
    "    time_series_data = df[passengers_col_name]\n",
    "    print(\"Dataset de ejemplo creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEJKsJwdN5A2"
   },
   "source": [
    "## **3. Visualización del Dataset y Análisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "Visualizaremos la serie temporal para identificar patrones como tendencia, estacionalidad y posibles anomalías.\n",
    "También verificaremos si hay valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1746647639864,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "HBa2PK_ZN8Pf",
    "outputId": "bd71655d-f637-435c-e1cc-e229014edfe8"
   },
   "outputs": [],
   "source": [
    "# 3. Visualización del Dataset y EDA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if 'time_series_data' in locals():\n",
    "    # 3.1 Visualización de la Serie Temporal Completa\n",
    "    print(\"\\n--- 3.1 Visualización de la Serie Temporal Completa ---\")\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(time_series_data.index, time_series_data.values, label=f'Número de {passengers_col_name}')\n",
    "    plt.title(f'Serie Temporal: {passengers_col_name} (1949-1960)', fontsize=16)\n",
    "    plt.xlabel('Fecha', fontsize=12)\n",
    "    plt.ylabel(f'Número de {passengers_col_name}', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    # Formatear el eje x para mostrar años\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator(2)) # Marcas cada 2 años\n",
    "    plt.show()\n",
    "\n",
    "    # 3.2 Verificación de Valores Faltantes\n",
    "    print(\"\\n--- 3.2 Verificación de Valores Faltantes ---\")\n",
    "    missing_values = time_series_data.isnull().sum()\n",
    "    print(f\"Número de valores faltantes en la serie: {missing_values}\")\n",
    "    if missing_values > 0:\n",
    "        print(\"Advertencia: Existen valores faltantes. Se deberían tratar (ej. interpolación, eliminación).\")\n",
    "        # Para este ejemplo, no implementaremos el tratamiento de faltantes.\n",
    "else:\n",
    "    print(\"No se pudo cargar `time_series_data`. Saltando EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CJe7ZZFN_IG"
   },
   "source": [
    "## **4. Preprocesamiento del Dataset**\n",
    "\n",
    "El preprocesamiento para modelos RNN de series temporales generalmente incluye:\n",
    "1.  **Escalado de Datos:** Los modelos de redes neuronales suelen funcionar mejor con datos escalados (ej. entre 0 y 1 o -1 y 1). Usaremos `MinMaxScaler`.\n",
    "2.  **División en Entrenamiento y Prueba:** Es crucial que la división sea cronológica para series temporales. No se deben mezclar datos del futuro en el conjunto de entrenamiento.\n",
    "3.  **Creación de Secuencias:** Las RNNs (LSTM/GRU) procesan secuencias de datos. Necesitamos transformar nuestra serie temporal en pares de (secuencia_de_entrada, valor_objetivo).\n",
    "    * `sequence_length` (o lookback): Número de pasos temporales pasados que se usarán como entrada.\n",
    "    * `target_steps` (o horizon): Número de pasos futuros a predecir (aquí, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746647639868,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "ACKB8OGRN_z3",
    "outputId": "6b2683ad-4b19-4527-bda9-dc4fd89ee99e"
   },
   "outputs": [],
   "source": [
    "# 4. Preprocesamiento del Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if 'time_series_data' in locals():\n",
    "    # 4.1 Escalado de Datos\n",
    "    print(\"\\n--- 4.1 Escalado de Datos ---\")\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) # Escalar a [-1, 1] es común para LSTMs\n",
    "    # scaler = MinMaxScaler(feature_range=(0, 1)) # Escalar a [0, 1] también es una opción\n",
    "\n",
    "    # El método fit_transform espera un array 2D, así que convertimos la serie 1D\n",
    "    data_scaled = scaler.fit_transform(time_series_data.values.reshape(-1, 1))\n",
    "    print(f\"Forma de los datos escalados: {data_scaled.shape}\")\n",
    "    print(f\"Primeros 5 valores escalados:\\n{data_scaled[:5]}\")\n",
    "\n",
    "    # 4.2 División en Entrenamiento y Prueba (Cronológica)\n",
    "    print(\"\\n--- 4.2 División en Entrenamiento y Prueba ---\")\n",
    "    # Por ejemplo, usar el 80% para entrenamiento y el 20% para prueba\n",
    "    train_size = int(len(data_scaled) * 0.80)\n",
    "    test_size = len(data_scaled) - train_size\n",
    "\n",
    "    train_data_scaled = data_scaled[0:train_size, :]\n",
    "    test_data_scaled = data_scaled[train_size:len(data_scaled), :]\n",
    "\n",
    "    print(f\"Tamaño del conjunto de entrenamiento escalado: {len(train_data_scaled)}\")\n",
    "    print(f\"Tamaño del conjunto de prueba escalado: {len(test_data_scaled)}\")\n",
    "\n",
    "    # 4.3 Creación de Secuencias\n",
    "    print(\"\\n--- 4.3 Creación de Secuencias ---\")\n",
    "    def create_sequences(input_data, sequence_length, target_steps=1):\n",
    "        \"\"\"\n",
    "        Crea secuencias de entrada y etiquetas objetivo para modelos RNN.\n",
    "        Args:\n",
    "            input_data (np.array): Array de la serie temporal (ya escalada).\n",
    "            sequence_length (int): Número de pasos temporales pasados a usar como entrada.\n",
    "            target_steps (int): Número de pasos futuros a predecir. Default 1.\n",
    "        Returns:\n",
    "            tuple: (np.array de secuencias de entrada, np.array de etiquetas objetivo)\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(input_data) - sequence_length - (target_steps - 1)):\n",
    "            # La secuencia de entrada\n",
    "            seq_in = input_data[i:(i + sequence_length), 0] # Tomar 'sequence_length' valores\n",
    "            # La etiqueta objetivo (el/los siguiente/s valor/es)\n",
    "            seq_out = input_data[(i + sequence_length):(i + sequence_length + target_steps), 0]\n",
    "            X.append(seq_in)\n",
    "            y.append(seq_out)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    SEQUENCE_LENGTH = 12  # Usar los últimos 12 meses para predecir el siguiente\n",
    "    TARGET_STEPS = 1      # Predecir 1 mes adelante\n",
    "\n",
    "    X_train, y_train = create_sequences(train_data_scaled, SEQUENCE_LENGTH, TARGET_STEPS)\n",
    "    X_test, y_test = create_sequences(test_data_scaled, SEQUENCE_LENGTH, TARGET_STEPS)\n",
    "\n",
    "    print(f\"Forma de X_train: {X_train.shape}\") # Debería ser (num_samples, sequence_length)\n",
    "    print(f\"Forma de y_train: {y_train.shape}\")   # Debería ser (num_samples, target_steps)\n",
    "    print(f\"Forma de X_test: {X_test.shape}\")\n",
    "    print(f\"Forma de y_test: {y_test.shape}\")\n",
    "\n",
    "    # Convertir a Tensores PyTorch\n",
    "    # RNNs en PyTorch esperan la entrada como (seq_len, batch, input_size) o (batch, seq_len, input_size)\n",
    "    # Si batch_first=True en la capa RNN, entonces (batch, seq_len, input_size)\n",
    "    # Nuestra X_train/X_test es (num_samples, sequence_length). Necesitamos añadir la dimensión de 'features'.\n",
    "    # Para series univariadas, input_size (num_features) es 1.\n",
    "    X_train_tensor = torch.from_numpy(X_train).float().unsqueeze(-1) # (num_samples, seq_len, 1)\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(-1) # (num_samples, target_steps, 1)\n",
    "    X_test_tensor = torch.from_numpy(X_test).float().unsqueeze(-1)\n",
    "    y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(-1)\n",
    "    # Si target_steps es 1 y la última capa lineal tiene 1 salida, y_train/y_test puede ser solo (num_samples, 1)\n",
    "    # y no (num_samples, 1, 1). Vamos a ajustar y_train/y_test si TARGET_STEPS = 1.\n",
    "    if TARGET_STEPS == 1:\n",
    "        y_train_tensor = y_train_tensor.squeeze(-1) # (num_samples, 1)\n",
    "        y_test_tensor = y_test_tensor.squeeze(-1)   # (num_samples, 1)\n",
    "\n",
    "\n",
    "    print(f\"\\nForma de X_train_tensor: {X_train_tensor.shape}\")\n",
    "    print(f\"Forma de y_train_tensor: {y_train_tensor.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"No se pudo cargar `time_series_data`. Saltando Preprocesamiento.\")\n",
    "    # Definir tensores vacíos para que el resto del notebook no falle catastróficamente\n",
    "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "\n",
    "print(\"\\n--- Fin de Preprocesamiento ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnaLnjAJODw2"
   },
   "source": [
    "## **5. Creación de los `DataLoader` de PyTorch**\n",
    "\n",
    "Con las secuencias de entrenamiento y prueba ya convertidas a tensores PyTorch, crearemos `Dataset` personalizados y luego los `DataLoader` correspondientes.\n",
    "\n",
    "Esto facilitará la iteración por lotes (batches) durante el entrenamiento y la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746647639873,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "8kxoYjPNOGww",
    "outputId": "705eb46a-16b6-452d-8065-9351444e3810"
   },
   "outputs": [],
   "source": [
    "# 5. Creación de los DataLoader\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para secuencias de series temporales.\"\"\"\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.y_data[idx]\n",
    "\n",
    "# X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor deberían estar definidos\n",
    "# en la Celda 8 (Preprocesamiento)\n",
    "if 'X_train_tensor' in locals() and X_train_tensor.nelement() > 0:\n",
    "    train_dataset_ts = TimeSeriesDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset_ts = TimeSeriesDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Aquí se usa BATCH_SIZE. Si Celda 4 no se ejecutó, dará error.\n",
    "    train_dataloader_ts = DataLoader(train_dataset_ts, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    test_dataloader_ts = DataLoader(test_dataset_ts, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    print(f\"DataLoaders `train_dataloader_ts` y `test_dataloader_ts` creados.\")\n",
    "    print(f\"Número de lotes en train_dataloader_ts: {len(train_dataloader_ts)}\")\n",
    "    print(f\"Número de lotes en test_dataloader_ts: {len(test_dataloader_ts)}\")\n",
    "\n",
    "    # Verificar un lote (opcional)\n",
    "    # x_batch_check, y_batch_check = next(iter(train_dataloader_ts))\n",
    "    # print(f\"Forma del lote de X de entrenamiento: {x_batch_check.shape}\")\n",
    "    # print(f\"Forma del lote de y de entrenamiento: {y_batch_check.shape}\")\n",
    "else:\n",
    "    print(\"Tensores de entrenamiento/prueba están vacíos o no definidos \"\n",
    "          \"(probablemente por fallo en carga de datos o preprocesamiento). \"\n",
    "          \"No se pueden crear DataLoaders.\")\n",
    "    train_dataloader_ts, test_dataloader_ts = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4N5Qt3rOKo-"
   },
   "source": [
    "## **6. Definición de Modelos RNN (LSTM y GRU)**\n",
    "\n",
    "Definiremos dos modelos basados en RNN:\n",
    "1.  **Modelo LSTM:** Utilizará una o más capas LSTM seguidas de una capa lineal para la predicción.\n",
    "2.  **Modelo GRU:** Similar, pero utilizando capas GRU.\n",
    "\n",
    "Ambos modelos tomarán secuencias de entrada y predecirán el valor del siguiente paso temporal.\n",
    "Las capas LSTM y GRU en PyTorch esperan la entrada con `batch_first=True` como `(batch_size, seq_len, input_size/num_features)`.\n",
    "Nuestra `input_size` (número de features por paso temporal) es 1 para esta serie univariada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1746647639898,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "f38TXtVNONT_",
    "outputId": "862dd536-f4c3-4c7d-fdf4-03a6bf345f56"
   },
   "outputs": [],
   "source": [
    "# 6. Definición de Modelos RNN (LSTM y GRU)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "INPUT_SIZE_RNN = 1 # Número de features en cada paso temporal (1 para univariada)\n",
    "HIDDEN_SIZE_RNN = 64 # Número de neuronas en la capa oculta de LSTM/GRU\n",
    "NUM_LAYERS_RNN = 1   # Número de capas recurrentes apiladas\n",
    "OUTPUT_SIZE_RNN = TARGET_STEPS # Predecir `TARGET_STEPS` adelante (aquí 1)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_s, hidden_s, num_l, output_s, dropout_p=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_s\n",
    "        self.num_layers = num_l\n",
    "\n",
    "        self.lstm = nn.LSTM(input_s, hidden_s, num_l, batch_first=True, dropout=dropout_p if num_l > 1 else 0)\n",
    "        # batch_first=True -> la entrada y salida son (batch, seq, feature)\n",
    "        # dropout solo se aplica si num_layers > 1\n",
    "\n",
    "        self.fc = nn.Linear(hidden_s, output_s) # Capa lineal para la salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inicializar estado oculto y estado de celda\n",
    "        # h0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # c0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decodificar el estado oculto del último paso temporal\n",
    "        # Queremos la salida del último paso de la secuencia para la predicción\n",
    "        out = self.fc(out[:, -1, :]) # out[:, -1, :] da (batch_size, hidden_size)\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_s, hidden_s, num_l, output_s, dropout_p=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_s\n",
    "        self.num_layers = num_l\n",
    "\n",
    "        self.gru = nn.GRU(input_s, hidden_s, num_l, batch_first=True, dropout=dropout_p if num_l > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_s, output_s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Instanciar los modelos\n",
    "modelo_lstm_pt = LSTMModel(INPUT_SIZE_RNN, HIDDEN_SIZE_RNN, NUM_LAYERS_RNN, OUTPUT_SIZE_RNN)\n",
    "modelo_gru_pt = GRUModel(INPUT_SIZE_RNN, HIDDEN_SIZE_RNN, NUM_LAYERS_RNN, OUTPUT_SIZE_RNN)\n",
    "\n",
    "print(\"--- Modelo LSTM (PyTorch) ---\")\n",
    "print(modelo_lstm_pt)\n",
    "# from torchinfo import summary\n",
    "# if X_train_tensor.nelement() > 0: # Solo si hay datos para inferir tamaño de lote\n",
    "#     summary(modelo_lstm_pt, input_size=(BATCH_SIZE, SEQUENCE_LENGTH, INPUT_SIZE_RNN), device=\"cpu\")\n",
    "\n",
    "print(\"\\n--- Modelo GRU (PyTorch) ---\")\n",
    "print(modelo_gru_pt)\n",
    "# if X_train_tensor.nelement() > 0:\n",
    "#     summary(modelo_gru_pt, input_size=(BATCH_SIZE, SEQUENCE_LENGTH, INPUT_SIZE_RNN), device=\"cpu\")\n",
    "\n",
    "print(\"\\nModelos RNN (PyTorch) definidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0H61XooOPcO"
   },
   "source": [
    "## **7. Entrenamiento y Evaluación de los Modelos (PyTorch)**\n",
    "\n",
    "El proceso será similar al del notebook de CNNs en PyTorch:\n",
    "1.  Definir una función de pérdida: `nn.MSELoss` (Error Cuadrático Medio), común para regresión.\n",
    "2.  Definir un optimizador: `optim.Adam`.\n",
    "3.  Crear funciones para el bucle de entrenamiento y el de evaluación.\n",
    "4.  Entrenar y evaluar ambos modelos (LSTM y GRU).\n",
    "5.  Graficar las curvas de pérdida de entrenamiento y validación.\n",
    "\n",
    "Las métricas principales serán MSE y RMSE (Raíz del Error Cuadrático Medio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9635,
     "status": "ok",
     "timestamp": 1746647649535,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "sMaXfKolOUt9",
    "outputId": "0ea411a5-4b4c-417d-895f-0b649414e087"
   },
   "outputs": [],
   "source": [
    "# 7. Entrenamiento y Evaluación de los Modelos RNN (PyTorch)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "criterion_ts = nn.MSELoss()\n",
    "\n",
    "# Las funciones `train_one_epoch` y `evaluate_model_pytorch` definidas para CNN\n",
    "# son genéricas y deberían funcionar aquí también, siempre que la forma de\n",
    "# las salidas del modelo y las etiquetas coincidan con lo que espera `criterion_ts`.\n",
    "# Aquí, y_train_tensor es (batch, 1) y la salida del modelo es (batch, 1).\n",
    "\n",
    "# Modificamos ligeramente `train_one_epoch` y `evaluate_model_pytorch` para no calcular accuracy,\n",
    "# ya que es un problema de regresión. Retornaremos solo la pérdida.\n",
    "def train_one_epoch_ts(model, dataloader, criterion, optimizer, device, epoch_num, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    print_interval = max(1, len(dataloader) // 5)\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0) # Ponderar por tamaño de lote\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            print(f\"    Época [{epoch_num+1}/{total_epochs}], Lote [{batch_idx+1}/{len(dataloader)}], Pérdida Lote: {loss.item():.6f}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate_model_ts(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    eval_loss = running_loss / total_samples\n",
    "    return eval_loss, torch.cat(all_outputs), torch.cat(all_labels)\n",
    "\n",
    "\n",
    "def entrenar_y_evaluar_rnn_pytorch(modelo_rnn, train_dl, test_dl, criterio_loss, optimizador_rnn, num_epochs, nombre_modelo_str, dev):\n",
    "    print(f\"\\n--- Entrenando y Evaluando: {nombre_modelo_str} en {dev} ---\")\n",
    "    modelo_rnn.to(dev)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_info_rnn = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Iniciando Época {epoch+1}/{num_epochs} para {nombre_modelo_str}\")\n",
    "        train_loss = train_one_epoch_ts(modelo_rnn, train_dl, criterio_loss, optimizador_rnn, dev, epoch, num_epochs)\n",
    "        val_loss, _, _ = evaluate_model_ts(modelo_rnn, test_dl, criterio_loss, dev) # No necesitamos outputs/labels aquí\n",
    "\n",
    "        print(f\"  FIN ÉPOCA {epoch+1}/{num_epochs}: Pérdida Entr: {train_loss:.6f}, Pérdida Val: {val_loss:.6f}\")\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch_info_rnn = {'epoch': epoch + 1, 'val_loss': val_loss}\n",
    "            # torch.save(modelo_rnn.state_dict(), f'{nombre_modelo_str}_best_ts.pth') # Opcional\n",
    "            print(f\"    Nueva mejor Pérdida Validación: {best_val_loss:.6f}\")\n",
    "\n",
    "    # Graficar historial de pérdida\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_loss'], label='Pérdida Entrenamiento')\n",
    "    plt.plot(history['val_loss'], label='Pérdida Validación')\n",
    "    plt.title(f\"Historial de Pérdida: {nombre_modelo_str}\", fontsize=16)\n",
    "    plt.xlabel('Época', fontsize=12); plt.ylabel('Pérdida (MSE)', fontsize=12)\n",
    "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.7); plt.show()\n",
    "\n",
    "    final_val_loss, final_val_rmse = best_epoch_info_rnn.get('val_loss', float('inf')), math.sqrt(best_epoch_info_rnn.get('val_loss', float('inf')))\n",
    "    print(f\"\\nMejor resultado para {nombre_modelo_str} en época {best_epoch_info_rnn.get('epoch', 'N/A')}: \"\n",
    "          f\"Pérdida Val (MSE) = {final_val_loss:.6f}, RMSE Val = {final_val_rmse:.4f}\")\n",
    "\n",
    "    return modelo_rnn, history, final_val_loss, final_val_rmse\n",
    "\n",
    "\n",
    "EPOCHS_RNN = 150 # Aumentar para mejores resultados, ej. 100-200\n",
    "LEARNING_RATE_RNN = 0.001\n",
    "\n",
    "if train_dataloader_ts is not None:\n",
    "    # Entrenar Modelo LSTM\n",
    "    optimizer_lstm_pt = optim.Adam(modelo_lstm_pt.parameters(), lr=LEARNING_RATE_RNN)\n",
    "    modelo_lstm_pt, hist_lstm_pt, loss_lstm_pt, rmse_lstm_pt = entrenar_y_evaluar_rnn_pytorch(\n",
    "        modelo_lstm_pt, train_dataloader_ts, test_dataloader_ts, criterion_ts, optimizer_lstm_pt,\n",
    "        EPOCHS_RNN, \"LSTM (PyTorch)\", device\n",
    "    )\n",
    "\n",
    "    # Entrenar Modelo GRU\n",
    "    optimizer_gru_pt = optim.Adam(modelo_gru_pt.parameters(), lr=LEARNING_RATE_RNN)\n",
    "    modelo_gru_pt, hist_gru_pt, loss_gru_pt, rmse_gru_pt = entrenar_y_evaluar_rnn_pytorch(\n",
    "        modelo_gru_pt, train_dataloader_ts, test_dataloader_ts, criterion_ts, optimizer_gru_pt,\n",
    "        EPOCHS_RNN, \"GRU (PyTorch)\", device\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Resumen de Resultados (PyTorch - Series Temporales) ---\")\n",
    "    print(f\"LSTM: Mejor Pérdida Val (MSE) = {loss_lstm_pt:.6f}, RMSE Val = {rmse_lstm_pt:.4f}\")\n",
    "    print(f\"GRU:  Mejor Pérdida Val (MSE) = {loss_gru_pt:.6f}, RMSE Val = {rmse_gru_pt:.4f}\")\n",
    "\n",
    "    # Seleccionar el mejor modelo\n",
    "    if loss_lstm_pt < loss_gru_pt: # Menor MSE es mejor\n",
    "        mejor_modelo_rnn_pt = modelo_lstm_pt\n",
    "        nombre_mejor_modelo_rnn_pt = \"LSTM (PyTorch)\"\n",
    "        rmse_mejor_modelo_rnn_pt = rmse_lstm_pt\n",
    "    else:\n",
    "        mejor_modelo_rnn_pt = modelo_gru_pt\n",
    "        nombre_mejor_modelo_rnn_pt = \"GRU (PyTorch)\"\n",
    "        rmse_mejor_modelo_rnn_pt = rmse_gru_pt\n",
    "    print(f\"\\nEl mejor modelo RNN (PyTorch) es: {nombre_mejor_modelo_rnn_pt} con RMSE Val = {rmse_mejor_modelo_rnn_pt:.4f}\")\n",
    "else:\n",
    "    print(\"No se pudieron entrenar los modelos RNN debido a DataLoaders vacíos.\")\n",
    "    mejor_modelo_rnn_pt, nombre_mejor_modelo_rnn_pt = None, \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGdqs9dUOWlG"
   },
   "source": [
    "## **8. Inferencia (Previsión) con el Mejor Modelo (PyTorch)**\n",
    "\n",
    "Usaremos el mejor modelo RNN entrenado para hacer previsiones sobre el conjunto de prueba.\n",
    "\n",
    "Los pasos son:\n",
    "1. Poner el modelo en modo `model.eval()`.\n",
    "2. Iterar sobre el `test_dataloader_ts`.\n",
    "3. Realizar predicciones.\n",
    "4. **Importante:** Desescalar las predicciones y los valores verdaderos para interpretarlos en la escala original de pasajeros.\n",
    "5. Graficar las predicciones contra los valores reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1746647650063,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "YfLhiLAbOfgm",
    "outputId": "af0f7c70-425a-4f75-c2bb-90b37d3f5a97"
   },
   "outputs": [],
   "source": [
    "# 8. Inferencia (Previsión) con el Mejor Modelo (PyTorch)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if mejor_modelo_rnn_pt is not None and test_dataloader_ts is not None and 'scaler' in locals():\n",
    "    print(f\"Usando: {nombre_mejor_modelo_rnn_pt} para la previsión.\")\n",
    "    mejor_modelo_rnn_pt.eval() # Modo evaluación\n",
    "\n",
    "    # Obtener todas las predicciones y etiquetas del conjunto de prueba\n",
    "    _, test_preds_scaled_tensor, test_labels_scaled_tensor = evaluate_model_ts(\n",
    "        mejor_modelo_rnn_pt, test_dataloader_ts, criterion_ts, device\n",
    "    )\n",
    "\n",
    "    # Convertir a NumPy y desescalar\n",
    "    test_preds_scaled_np = test_preds_scaled_tensor.numpy()\n",
    "    test_labels_scaled_np = test_labels_scaled_tensor.numpy()\n",
    "\n",
    "    # `scaler` fue ajustado con .values.reshape(-1, 1), así que las predicciones también necesitan esa forma\n",
    "    # y luego se desescalan.\n",
    "    # Si y_test fue (num_samples, 1), entonces test_preds_scaled_np y test_labels_scaled_np también lo son.\n",
    "    test_preds_descaled = scaler.inverse_transform(test_preds_scaled_np)\n",
    "    test_labels_descaled = scaler.inverse_transform(test_labels_scaled_np)\n",
    "\n",
    "    # Calcular RMSE en la escala original\n",
    "    rmse_original_scale = math.sqrt(mean_squared_error(test_labels_descaled, test_preds_descaled))\n",
    "    print(f\"RMSE de previsión en escala original: {rmse_original_scale:.2f} pasajeros\")\n",
    "\n",
    "    # Graficar las previsiones vs los valores reales\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(test_labels_descaled, label='Valores Reales (Prueba)', color='blue', marker='.')\n",
    "    plt.plot(test_preds_descaled, label='Predicciones del Modelo (Prueba)', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Previsión de Pasajeros: {nombre_mejor_modelo_rnn_pt}', fontsize=16)\n",
    "    plt.xlabel(f'Índice de Tiempo (en conjunto de prueba, desde {len(train_data_scaled) - SEQUENCE_LENGTH})', fontsize=12)\n",
    "    plt.ylabel(f'Número de {passengers_col_name}', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # Graficar sobre la serie temporal original (parte de prueba)\n",
    "    # Necesitamos los índices de fecha correctos para la parte de prueba\n",
    "    test_dates = time_series_data.index[train_size + SEQUENCE_LENGTH : train_size + SEQUENCE_LENGTH + len(test_labels_descaled)]\n",
    "\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(time_series_data.index, time_series_data.values, label=\"Serie Original Completa\", alpha=0.5)\n",
    "    plt.plot(test_dates, test_labels_descaled, label='Valores Reales (Prueba)', color='blue', marker='.')\n",
    "    plt.plot(test_dates, test_preds_descaled, label='Predicciones del Modelo (Prueba)', color='red', linestyle='--')\n",
    "    plt.title(f'Previsión Superpuesta en Serie Original: {nombre_mejor_modelo_rnn_pt}', fontsize=16)\n",
    "    plt.xlabel('Fecha', fontsize=12)\n",
    "    plt.ylabel(f'Número de {passengers_col_name}', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=12)) # Marcas cada 12 meses\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se puede realizar inferencia: modelo no entrenado, dataloader de prueba no disponible o scaler no definido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAUCEEEYOihu"
   },
   "source": [
    "## **9. Guardar el Mejor Modelo (PyTorch)**\n",
    "\n",
    "Guardaremos el `state_dict` del mejor modelo RNN entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746647650071,
     "user": {
      "displayName": "Santiago Vasquez",
      "userId": "18376430897174478567"
     },
     "user_tz": 180
    },
    "id": "MXKxeIF1Ok4u",
    "outputId": "4689bc40-c378-4456-cfd2-ffd4476ead0e"
   },
   "outputs": [],
   "source": [
    "# 9. Guardar el Mejor Modelo (PyTorch)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if mejor_modelo_rnn_pt is not None:\n",
    "    modelos_rnn_guardados_dir_pt = \"modelos_rnn_pytorch_guardados\"\n",
    "    if not os.path.exists(modelos_rnn_guardados_dir_pt):\n",
    "        os.makedirs(modelos_rnn_guardados_dir_pt)\n",
    "\n",
    "    timestamp_rnn_pt = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    nombre_archivo_limpio_rnn_pt = nombre_mejor_modelo_rnn_pt.replace(\" (PyTorch)\", \"\").replace(\" \", \"_\")\n",
    "    nombre_archivo_modelo_rnn_pt = f\"{nombre_archivo_limpio_rnn_pt}_{timestamp_rnn_pt}_ts.pth\"\n",
    "    ruta_guardado_modelo_rnn_pt = os.path.join(modelos_rnn_guardados_dir_pt, nombre_archivo_modelo_rnn_pt)\n",
    "\n",
    "    print(f\"Guardando el state_dict del mejor modelo ({nombre_mejor_modelo_rnn_pt}) en: {ruta_guardado_modelo_rnn_pt}\")\n",
    "    torch.save(mejor_modelo_rnn_pt.state_dict(), ruta_guardado_modelo_rnn_pt)\n",
    "    print(\"State_dict del modelo RNN guardado exitosamente.\")\n",
    "\n",
    "    # print(\"\\nEjemplo de cómo cargar el modelo RNN (comentado):\")\n",
    "    # if nombre_mejor_modelo_rnn_pt == \"LSTM (PyTorch)\":\n",
    "    #     modelo_cargado_rnn_pt = LSTMModel(INPUT_SIZE_RNN, HIDDEN_SIZE_RNN, NUM_LAYERS_RNN, OUTPUT_SIZE_RNN)\n",
    "    # else: # Asumir GRU\n",
    "    #     modelo_cargado_rnn_pt = GRUModel(INPUT_SIZE_RNN, HIDDEN_SIZE_RNN, NUM_LAYERS_RNN, OUTPUT_SIZE_RNN)\n",
    "    #\n",
    "    # modelo_cargado_rnn_pt.load_state_dict(torch.load(ruta_guardado_modelo_rnn_pt, map_location=device))\n",
    "    # modelo_cargado_rnn_pt.to(device)\n",
    "    # modelo_cargado_rnn_pt.eval()\n",
    "    # print(f\"Modelo RNN '{nombre_mejor_modelo_rnn_pt}' cargado y listo para inferencia en {device}.\")\n",
    "else:\n",
    "    print(\"No hay un mejor modelo RNN para guardar (posiblemente por fallo en carga de datos o entrenamiento).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOdXUmvLOva-"
   },
   "source": [
    "## **Conclusiones y Próximos Pasos (RNN para Series Temporales en PyTorch)**\n",
    "\n",
    "En este notebook, hemos construido un sistema de previsión de series temporales usando LSTMs y GRUs en PyTorch:\n",
    "1.  Cargamos y exploramos el dataset \"Air Passengers\".\n",
    "2.  Preprocesamos los datos: escalado y creación de secuencias de entrada-salida.\n",
    "3.  Definimos `Dataset` y `DataLoader` de PyTorch para manejar los datos.\n",
    "4.  Implementamos modelos LSTM y GRU.\n",
    "5.  Creamos bucles de entrenamiento y evaluación, y entrenamos los modelos.\n",
    "6.  Realizamos previsiones y las comparamos con los valores reales, desescalando para la interpretación.\n",
    "7.  Guardamos el `state_dict` del mejor modelo.\n",
    "\n",
    "**Posibles Próximos Pasos:**\n",
    "* **Multivariado:** Adaptar para series temporales multivariadas (múltiples features de entrada).\n",
    "* **Múltiples Pasos Futuros:** Modificar los modelos y la creación de secuencias para predecir más de un paso adelante a la vez.\n",
    "* **Atención:** Incorporar mecanismos de atención en los modelos RNN.\n",
    "* **Modelos Transformer:** Explorar arquitecturas basadas en Transformers, que han demostrado ser muy efectivas para secuencias.\n",
    "* **Hiperparámetros:** Ajustar `SEQUENCE_LENGTH`, `HIDDEN_SIZE_RNN`, `NUM_LAYERS_RNN`, tasa de aprendizaje, optimizador, etc.\n",
    "* **Análisis de Errores:** Investigar más a fondo los errores de predicción (ej. residuos).\n",
    "* **Incorporar Features Exógenas:** Si hay variables adicionales que puedan influir en la serie temporal (ej. festivos, promociones)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPWGZac7vbqIfo36v9NtsBO",
   "gpuType": "T4",
   "mount_file_id": "1LGLT5uFWaDNL5xjgYqf_4f5Yj3hFPXTD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
