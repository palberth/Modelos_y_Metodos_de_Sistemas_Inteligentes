{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f9a3a1",
   "metadata": {},
   "source": [
    "# Clasificación de Especies de Iris con Redes Neuronales (Keras) y Regularización\n",
    "\n",
    "**Disciplina:** Aprendizaje Profundo, Redes Neuronales, Clasificación, Keras (TensorFlow)\n",
    "\n",
    "**Objetivo:**\n",
    "El objetivo de este notebook es construir, entrenar y evaluar una red neuronal para clasificar las especies de flores del dataset Iris utilizando Keras. Se incorporarán técnicas de preprocesamiento, regularización (L2 y Dropout) y callbacks de Keras (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau) para mejorar el entrenamiento y la robustez del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcc612",
   "metadata": {},
   "source": [
    "## 1. Carga de Librerías y Configuración Inicial\n",
    "\n",
    "**Propósito de esta sección:**\n",
    "Importar todas las bibliotecas necesarias y configurar el entorno para el análisis, incluyendo la fijación de semillas para reproducibilidad.\n",
    "\n",
    "**Bibliotecas Clave:**\n",
    "* **`numpy`, `pandas`**: Para manipulación de datos.\n",
    "* **`matplotlib.pyplot`, `seaborn`**: Para visualizaciones.\n",
    "* **`sklearn.datasets`**: Para cargar el dataset Iris.\n",
    "* **`sklearn.model_selection`**: Para `train_test_split`.\n",
    "* **`sklearn.preprocessing`**: Para `StandardScaler` y `LabelEncoder` (aunque `to_categorical` maneja bien las clases numéricas).\n",
    "* **`sklearn.metrics`**: Para `classification_report` y `confusion_matrix`.\n",
    "* **`tensorflow.keras`**: Para construir y entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfecf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comandos mágicos de IPython (opcional en scripts)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb97ec8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Importación de bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Configuración para reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Configuración de estilo y visualización\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aed8c5",
   "metadata": {},
   "source": [
    "## 2. Funciones Personalizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e936e90",
   "metadata": {},
   "source": [
    "### Descripción de la Función: `cargar_y_preparar_datos_iris`\n",
    "\n",
    "**Objetivo Principal:**\n",
    "Cargar el dataset Iris, realizar preprocesamiento básico (escalado de características, codificación one-hot del objetivo) y dividirlo en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "**Características:**\n",
    "* **Procesamiento:**\n",
    "    1. Carga el dataset Iris.\n",
    "    2. Crea un DataFrame de Pandas para exploración (opcional).\n",
    "    3. Separa características (X) y objetivo (y).\n",
    "    4. Escala las características X usando `StandardScaler`.\n",
    "    5. Convierte las etiquetas y a formato categórico (one-hot encoding).\n",
    "    6. Divide los datos en conjuntos de entrenamiento y prueba.\n",
    "* **Valor de Retorno:**\n",
    "    * `X_train, X_test, y_train, y_test`: Conjuntos de datos divididos y preprocesados.\n",
    "    * `scaler`: El objeto `StandardScaler` ajustado (para posible uso en datos nuevos).\n",
    "    * `feature_names`, `target_names`: Nombres de características y clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39074392",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cargar_y_preparar_datos_iris(test_size=0.2, random_state=SEED):\n",
    "    \"\"\"\n",
    "    Carga, preprocesa y divide el dataset Iris.\n",
    "    \"\"\"\n",
    "    print(\"Cargando y preparando el dataset Iris...\")\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    feature_names = iris.feature_names\n",
    "    target_names = iris.target_names\n",
    "\n",
    "    # Opcional: Explorar con Pandas\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['species_code'] = y\n",
    "    df['species_name'] = df['species_code'].map({i: name for i, name in enumerate(target_names)})\n",
    "    print(\"\\nPrimeras filas del dataset Iris:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDistribución de clases:\")\n",
    "    print(df['species_name'].value_counts())\n",
    "\n",
    "    # Escalar características\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Codificación One-Hot para el objetivo\n",
    "    y_categorical = to_categorical(y)\n",
    "\n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_categorical, test_size=test_size, random_state=random_state, stratify=y # stratify para clasificación\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDimensiones: X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_names, target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadc94f",
   "metadata": {},
   "source": [
    "### Descripción de la Función: `crear_modelo_clasificacion_keras`\n",
    "\n",
    "**Objetivo Principal:**\n",
    "Definir y compilar un modelo de red neuronal secuencial con Keras para clasificación.\n",
    "\n",
    "**Características:**\n",
    "* **Entrada:**\n",
    "    * `input_dim` (int): Número de características de entrada.\n",
    "    * `output_dim` (int): Número de clases de salida.\n",
    "* **Procesamiento:**\n",
    "    1. Crea un modelo `Sequential`.\n",
    "    2. Añade capas `Dense` con activación 'relu', regularización L2 y Dropout.\n",
    "       (Considerar `BatchNormalization` opcionalmente).\n",
    "    3. Añade una capa de salida `Dense` con activación 'softmax'.\n",
    "    4. Compila el modelo con optimizador 'adam', pérdida 'categorical_crossentropy' y métrica 'accuracy'.\n",
    "* **Valor de Retorno:**\n",
    "    * `model` (tf.keras.Model): El modelo Keras compilado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bbff9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def crear_modelo_clasificacion_keras(input_dim, output_dim, l2_lambda=0.001, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal para clasificación con Keras.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreando el modelo de clasificación con Keras...\")\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        # BatchNormalization(), # Opcional: puede ayudar a estabilizar/acelerar\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        # BatchNormalization(), # Opcional\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(output_dim, activation='softmax') # Capa de salida para clasificación multiclase\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\nResumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd2fdf",
   "metadata": {},
   "source": [
    "### Descripción de la Función: `graficar_historial_entrenamiento`\n",
    "\n",
    "**Objetivo Principal:**\n",
    "Graficar las curvas de pérdida y precisión del entrenamiento y validación.\n",
    "\n",
    "**Características:**\n",
    "* **Entrada:**\n",
    "    * `history` (tf.keras.callbacks.History): Objeto devuelto por `model.fit()`.\n",
    "* **Procesamiento:**\n",
    "    * Extrae las métricas del historial.\n",
    "    * Crea subplots para la precisión y la pérdida.\n",
    "* **Salida:** Muestra los gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3698e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def graficar_historial_entrenamiento(history):\n",
    "    \"\"\"\n",
    "    Grafica la precisión y la pérdida durante el entrenamiento y validación.\n",
    "    \"\"\"\n",
    "    print(\"\\nGraficando historial de entrenamiento...\")\n",
    "    acc = history.history.get('accuracy')\n",
    "    val_acc = history.history.get('val_accuracy')\n",
    "    loss = history.history.get('loss')\n",
    "    val_loss = history.history.get('val_loss')\n",
    "    epochs_range = range(len(acc if acc else val_acc if val_acc else loss if loss else val_loss)) # Manejar si alguna métrica no está\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    if acc and val_acc:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, acc, label='Precisión (Entrenamiento)')\n",
    "        plt.plot(epochs_range, val_acc, label='Precisión (Validación)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Precisión de Entrenamiento y Validación')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Precisión')\n",
    "\n",
    "    if loss and val_loss:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, loss, label='Pérdida (Entrenamiento)')\n",
    "        plt.plot(epochs_range, val_loss, label='Pérdida (Validación)')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Pérdida de Entrenamiento y Validación')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Pérdida')\n",
    "    \n",
    "    if not (acc and val_acc and loss and val_loss): # Si faltan métricas para ambos gráficos\n",
    "        if acc and val_acc: # Solo hay precisión\n",
    "             plt.title('Precisión de Entrenamiento y Validación')\n",
    "             plt.xlabel('Épocas'); plt.ylabel('Precisión')\n",
    "        elif loss and val_loss: # Solo hay pérdida\n",
    "            plt.title('Pérdida de Entrenamiento y Validación')\n",
    "            plt.xlabel('Épocas'); plt.ylabel('Pérdida')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75b128",
   "metadata": {},
   "source": [
    "### Descripción de la Función: `evaluar_y_visualizar_clasificacion`\n",
    "\n",
    "**Objetivo Principal:**\n",
    "Evaluar el modelo de clasificación en el conjunto de prueba y visualizar la matriz de confusión.\n",
    "\n",
    "**Características:**\n",
    "* **Entrada:**\n",
    "    * `model` (tf.keras.Model): Modelo Keras entrenado.\n",
    "    * `X_test` (np.ndarray): Características del conjunto de prueba.\n",
    "    * `y_test_cat` (np.ndarray): Etiquetas one-hot del conjunto de prueba.\n",
    "    * `target_names` (list): Nombres de las clases.\n",
    "* **Procesamiento:**\n",
    "    1. Evalúa el modelo para obtener pérdida y precisión en el test set.\n",
    "    2. Realiza predicciones y las convierte de one-hot a etiquetas de clase.\n",
    "    3. Imprime el reporte de clasificación.\n",
    "    4. Genera y muestra la matriz de confusión.\n",
    "* **Salida:** Muestra métricas y gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_y_visualizar_clasificacion(model, X_test, y_test_cat, target_names):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo de clasificación y visualiza la matriz de confusión.\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluando el modelo en el conjunto de prueba...\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    print(f\"Pérdida en el conjunto de prueba: {loss:.4f}\")\n",
    "    print(f\"Precisión en el conjunto de prueba: {accuracy:.4f}\")\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "    # Reporte de Clasificación\n",
    "    print(\"\\nReporte de Clasificación:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=target_names, zero_division=0))\n",
    "\n",
    "    # Matriz de Confusión\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3eb41",
   "metadata": {},
   "source": [
    "## 3. Desarrollo del Ejercicio: Clasificación de Iris con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449f024",
   "metadata": {},
   "source": [
    "### 3.1. Carga y Preparación de Datos\n",
    "\n",
    "Cargamos el dataset Iris, lo escalamos y lo dividimos en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris, scaler_iris, iris_feature_names, iris_target_names = \\\n",
    "    cargar_y_preparar_datos_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb2825",
   "metadata": {},
   "source": [
    "### 3.2. Creación del Modelo de Clasificación\n",
    "\n",
    "Definimos la arquitectura de nuestra red neuronal usando Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_iris = X_train_iris.shape[1]\n",
    "output_dim_iris = y_train_iris.shape[1] # Número de clases (después de to_categorical)\n",
    "\n",
    "modelo_iris = crear_modelo_clasificacion_keras(input_dim_iris, output_dim_iris, l2_lambda=0.005, dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82343e",
   "metadata": {},
   "source": [
    "### 3.3. Entrenamiento del Modelo\n",
    "\n",
    "Entrenamos el modelo, utilizando callbacks para mejorar el proceso.\n",
    "* `EarlyStopping`: Detiene el entrenamiento si la pérdida de validación no mejora tras `patience` épocas.\n",
    "* `ModelCheckpoint`: Guarda el modelo con el mejor rendimiento en el conjunto de validación.\n",
    "* `ReduceLROnPlateau`: Reduce la tasa de aprendizaje si la pérdida de validación se estanca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e03875",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\nIniciando el entrenamiento del modelo Iris...\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_iris_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=0.00001, verbose=1)\n",
    "\n",
    "callbacks_list = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "# Entrenamiento\n",
    "history_iris = modelo_iris.fit(\n",
    "    X_train_iris, y_train_iris,\n",
    "    epochs=200, # Un número alto de épocas, EarlyStopping se encargará\n",
    "    batch_size=8, # Lotes pequeños para Iris\n",
    "    validation_split=0.2, # Usar una porción de los datos de entrenamiento para validación interna\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1 # 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    ")\n",
    "\n",
    "# Cargar el mejor modelo guardado por ModelCheckpoint (si restore_best_weights=False en EarlyStopping)\n",
    "# Si restore_best_weights=True en EarlyStopping, esto no es estrictamente necesario ya que el modelo ya tiene los mejores pesos.\n",
    "# modelo_iris = keras.models.load_model('best_iris_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd4e98",
   "metadata": {},
   "source": [
    "### 3.4. Visualización del Historial de Entrenamiento\n",
    "\n",
    "Observamos cómo evolucionaron la precisión y la pérdida durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if history_iris:\n",
    "    graficar_historial_entrenamiento(history_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6e5e4",
   "metadata": {},
   "source": [
    "### 3.5. Evaluación del Modelo\n",
    "\n",
    "Evaluamos el rendimiento del modelo entrenado en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelo_iris:\n",
    "    evaluar_y_visualizar_clasificacion(modelo_iris, X_test_iris, y_test_iris, iris_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937f6e2",
   "metadata": {},
   "source": [
    "## 4. Conclusiones del Ejercicio (Clasificación Iris)\n",
    "\n",
    "**Resumen de Hallazgos:**\n",
    "* Se cargó y preprocesó el dataset Iris, escalando las características y aplicando codificación one-hot a las etiquetas.\n",
    "* Se construyó una red neuronal secuencial con Keras, incorporando:\n",
    "    * Capas `Dense` con activación ReLU.\n",
    "    * Regularización L2 (`kernel_regularizer`) para penalizar pesos grandes.\n",
    "    * Capas `Dropout` para reducir el sobreajuste.\n",
    "    * Una capa de salida `softmax` adecuada para clasificación multiclase.\n",
    "* El modelo fue compilado con el optimizador `adam` y la función de pérdida `categorical_crossentropy`.\n",
    "* Durante el entrenamiento, se utilizaron callbacks:\n",
    "    * `EarlyStopping` para detener el entrenamiento prematuramente si no había mejora, previniendo el sobreajuste.\n",
    "    * `ModelCheckpoint` para guardar la mejor versión del modelo basada en `val_loss`.\n",
    "    * `ReduceLROnPlateau` para ajustar la tasa de aprendizaje dinámicamente.\n",
    "* La precisión final alcanzada en el conjunto de prueba fue de **[Completar con la precisión obtenida, ej: 0.97]**.\n",
    "* El reporte de clasificación y la matriz de confusión mostraron que el modelo fue capaz de **[Describir brevemente el rendimiento por clase, ej: clasificar correctamente la mayoría de las instancias, con algunas confusiones menores entre 'versicolor' y 'virginica']**.\n",
    "* Las curvas de aprendizaje (pérdida y precisión vs. épocas) indicaron **[Describir si hubo sobreajuste, si EarlyStopping actuó, etc.]**.\n",
    "\n",
    "**Aprendizaje General:**\n",
    "Este ejercicio demostró cómo construir una red neuronal para clasificación multiclase con Keras, aplicando buenas prácticas como el preprocesamiento de datos, la regularización para combatir el sobreajuste, y el uso de callbacks para un entrenamiento más eficiente y efectivo. La combinación de estas técnicas permite desarrollar modelos más robustos y con mejor capacidad de generalización.\n",
    "\n",
    "*(Nota: Los resultados específicos deben completarse después de ejecutar completamente el notebook.)*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
